{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required libraries\n",
    "import re\n",
    "import httpx\n",
    "from selectolax.parser import HTMLParser\n",
    "from dataclasses import dataclass, asdict, fields\n",
    "from urllib.parse import urljoin, urlsplit\n",
    "from typing import List, Dict, Optional\n",
    "import pandas as pd\n",
    "\n",
    "# Set the site url\n",
    "site_url: str = \"https://scotlandsplaces.gov.uk\"\n",
    "\n",
    "# Set the main url this county that you want to scrape\n",
    "main_url: str = \"https://scotlandsplaces.gov.uk/digital-volumes/ordnance-survey-name-books/lanarkshire-os-name-books-1858-1861\"\n",
    "\n",
    "# Set the csv file path\n",
    "gazetteer_filepath: str = \"./data/GazetteerNames.csv\"\n",
    "\n",
    "\n",
    "# Create a dataclass to store the information\n",
    "@dataclass\n",
    "class placename_information:\n",
    "    page_title: Optional[str] = \"No table on page\"\n",
    "    page_number: Optional[int] = 0\n",
    "    placename: str = \"No table on page\"\n",
    "    various_spellings: str = \"No table on page\"\n",
    "    authority: str = \"No table on page\"\n",
    "    situation: str = \"No table on page\"\n",
    "    description: str = \"No table on page\"\n",
    "    county: Optional[str] = \"No match\"\n",
    "\n",
    "\n",
    "# Create a function to extract the volume number from the url\n",
    "def extract_volume_number(url: str) -> int:\n",
    "    return int(url.split(\"-\")[-1].rstrip(\"/\"))\n",
    "\n",
    "\n",
    "# Create a function to get the volumes\n",
    "def get_volumes(url: str) -> list[str]:\n",
    "    try:\n",
    "        response = httpx.get(url)\n",
    "        response.raise_for_status()\n",
    "        print(f\"HTTP status: {response.status_code}\")\n",
    "    except httpx.HTTPStatusError as error:\n",
    "        print(f\"HTTP error occurred: {error}\")\n",
    "        return []\n",
    "\n",
    "    html = HTMLParser(response.text)\n",
    "    links = html.css(\"a\")\n",
    "\n",
    "    volume_numbers = []\n",
    "    for link in links:\n",
    "        href = link.attributes.get(\"href\")\n",
    "        if href:\n",
    "            match = re.search(\n",
    "                r\"/digital-volumes/ordnance-survey-name-books/lanarkshire-os-name-books-1858-1861/lanarkshire-volume-\\d+\",\n",
    "                href,\n",
    "            )\n",
    "            if match:\n",
    "                full_url = urljoin(site_url, match.group())\n",
    "                volume_numbers.append(full_url + \"/\")\n",
    "\n",
    "    volume_numbers.sort(key=extract_volume_number)\n",
    "\n",
    "    return volume_numbers\n",
    "\n",
    "\n",
    "# Create a function to get the page numbers\n",
    "def get_page_numbers(volume_url: str) -> list[int]:\n",
    "    response = httpx.get(volume_url)\n",
    "    html = HTMLParser(response.text)\n",
    "    links = html.css(\"a\")\n",
    "\n",
    "    page_numbers = []\n",
    "    for link in links:\n",
    "        href = link.attributes.get(\"href\")\n",
    "        if href:\n",
    "            match = re.search(r\"\\d+$\", href)\n",
    "            if match:\n",
    "                page_numbers.append(int(match.group()))\n",
    "\n",
    "    return list(set(page_numbers))\n",
    "\n",
    "\n",
    "# Create a function to get the html\n",
    "def get_html(volume_url: str, page: int) -> HTMLParser:\n",
    "    url = f\"{volume_url}{page}\"\n",
    "    response = httpx.get(url)\n",
    "    return HTMLParser(response.text)\n",
    "\n",
    "\n",
    "# Create a function to parse the placename information\n",
    "def parse_placename(html: HTMLParser, page: int) -> list[dict[str, str]]:\n",
    "    page_header = html.css_first(\"h1.page-header\").text()\n",
    "    name = html.css(\"div.well tr\")  # get each table row from table\n",
    "    results = []\n",
    "    for item in name:\n",
    "        item_parts = item.css(\"td\")\n",
    "        if len(item_parts) == 5:  # this will avoid the table headers\n",
    "            if any(\n",
    "                re.search(\n",
    "                    r\"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\",\n",
    "                    part.text().strip(),\n",
    "                )\n",
    "                for part in item_parts\n",
    "            ):\n",
    "                continue\n",
    "            new_item = placename_information(\n",
    "                page_title=page_header,\n",
    "                page_number=page,\n",
    "                placename=item_parts[0].text().strip(),\n",
    "                various_spellings=item_parts[1].text().replace(\"\\n\", \"|\").strip(),\n",
    "                authority=item_parts[2].text().replace(\"\\n\", \"|\").strip(),\n",
    "                situation=item_parts[3].text().strip(),\n",
    "                description=item_parts[4].text().strip(),\n",
    "                county=\"LAN\",\n",
    "            )\n",
    "            results.append(asdict(new_item))\n",
    "    # print(results)\n",
    "    return results\n",
    "\n",
    "# Get the county from the url\n",
    "def get_county_from_url(url: str) -> str:\n",
    "    split_url = urlsplit(url)\n",
    "    path_parts = split_url.path.split('/')\n",
    "    county = path_parts[3].split('-')[0].upper()\n",
    "    return county\n",
    "\n",
    "# Load csv file into a pandas dataframe and return the dataframe where local_authority is like the county\n",
    "def load_csv(filepath: str, county: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(filepath, low_memory=False)\n",
    "    df['local_authority'] = df['local_authority'].str.upper()\n",
    "    df = df[df['local_authority'].str.contains(county)]\n",
    "    return df   \n",
    "\n",
    "\n",
    "# Convert the list of dictionaries to a pandas dataframe\n",
    "def to_dataframe(res: List[Dict[str, str]]) -> pd.DataFrame:\n",
    "    fieldnames = [field.name for field in fields(placename_information)]\n",
    "    df = pd.DataFrame(res, columns=fieldnames)\n",
    "    return df\n",
    "\n",
    "\n",
    "# Output data to spreadsheet\n",
    "def write_to_excel(data: pd.DataFrame, output_filepath: str) -> None:\n",
    "    with pd.ExcelWriter(\n",
    "        output_filepath,\n",
    "        engine=\"xlsxwriter\",\n",
    "        datetime_format=\"dd mmm yyyy hh:mm:ss\",\n",
    "        date_format=\"dd mmm yyyy\",\n",
    "    ) as writer:\n",
    "        data.to_excel(\n",
    "            writer,\n",
    "            sheet_name=\"Sheet1\",\n",
    "            index=False,\n",
    "            header=False,\n",
    "            startrow=1,\n",
    "        )\n",
    "        workbook = writer.book\n",
    "        worksheet = writer.sheets[\"Sheet1\"]\n",
    "        header_format = workbook.add_format(\n",
    "            {\n",
    "                \"bold\": True,\n",
    "                \"text_wrap\": True,\n",
    "                \"valign\": \"top\",\n",
    "                \"fg_color\": \"#9FC5E8\",\n",
    "                \"border\": 1,\n",
    "            }\n",
    "        )\n",
    "        for col_num, value in enumerate(data.columns):\n",
    "            worksheet.write(0, col_num, value, header_format)\n",
    "        print(\"Spreadsheet created.\")\n",
    "\n",
    "\n",
    "# Create a main function\n",
    "def main() -> None:\n",
    "    county = get_county_from_url(main_url)\n",
    "    df = load_csv(gazetteer_filepath, county)\n",
    "\n",
    "    volumes = get_volumes(main_url)\n",
    "    print(volumes)\n",
    "    for volume in volumes:\n",
    "        volume_name = urlsplit(volume).path.split(\"/\")[-2]\n",
    "        page_numbers = get_page_numbers(volume)\n",
    "        print(page_numbers)\n",
    "        results = []  # Define results here\n",
    "        for page in page_numbers:\n",
    "            html = get_html(volume, page)\n",
    "            page_results = parse_placename(html, page)  # Pass page to parse_placename\n",
    "            print(f\"Results for volume {volume_name}, page {page}: {page_results}\")\n",
    "            results.extend(page_results)  # Add page_results to results\n",
    "        df = to_dataframe(results)  # Pass results to to_dataframe\n",
    "        write_to_excel(df, f\"./output/{volume_name}.xlsx\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
